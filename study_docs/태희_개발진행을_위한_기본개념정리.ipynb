{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25553922",
   "metadata": {},
   "source": [
    "# Baseline 코드 이해하기\n",
    "\n",
    "Baseline 코드는 GPT 모델을 활용하여 프롬프트 기반 응답을 생성하는 구조로 구성되어 있음.\n",
    "\n",
    "## 주요 구성 요소\n",
    "\n",
    "- **환경 변수 로드 (.env)**:\n",
    "  - API 키, 기본 설정 등을 환경 변수로 분리하여 보안성 확보\n",
    "  - `dotenv` 라이브러리를 통해 로드\n",
    "\n",
    "- **OpenAI 클라이언트 초기화**:\n",
    "  - `openai` 라이브러리 사용\n",
    "  - `OpenAI(api_key=...)`와 같이 객체 생성\n",
    "\n",
    "- **프롬프트 로드**:\n",
    "  - YAML 파일에서 system/user 프롬프트 불러오기\n",
    "  - 작업 목적에 따라 다양한 프롬프트 구성 가능\n",
    "\n",
    "- **GPT 호출 함수**:\n",
    "  - `call_gpt(prompt_key)` 형태로 GPT-4o API 호출\n",
    "  - 선택된 프롬프트를 기반으로 응답 생성\n",
    "  - 예외 처리 포함\n",
    "\n",
    "- **자동 실행 메인**:\n",
    "  - `if __name__ == \"__main__\":` 블록에서 기본 프롬프트 실행\n",
    "\n",
    "\n",
    "# LLM(Large Language Model)이란?\n",
    "\n",
    "## 정의\n",
    "- 대규모 데이터와 수십억 개의 파라미터를 기반으로 학습한 **자연어 처리 모델**\n",
    "- 문맥 이해, 생성, 추론 등 다양한 언어 작업 수행 가능\n",
    "\n",
    "## 기술 발전\n",
    "- N-gram → RNN → LSTM → Transformer\n",
    "- 현재는 **Transformer 기반 LLM**이 주류\n",
    "\n",
    "## 대표 모델\n",
    "- GPT (OpenAI)\n",
    "- Claude (Anthropic)\n",
    "- Gemini/PaLM (Google)\n",
    "- LLaMA (Meta)\n",
    "\n",
    "## 구성 요소\n",
    "- **Pretraining**: 대규모 코퍼스에서 일반 언어 패턴 학습\n",
    "- **Fine-tuning**: 특정 작업에 맞춰 추가 학습\n",
    "\n",
    "# LLM 기반 챗봇의 동작 과정\n",
    "\n",
    "1. **입력 처리**\n",
    "   - 사용자의 질문 또는 대화문 입력\n",
    "   - 토크나이저(tokenizer)를 통해 입력을 토큰화\n",
    "\n",
    "2. **프롬프트 구성**\n",
    "   - system/user 역할 지정\n",
    "   - 문맥 유지 및 목적에 맞는 프롬프트 설계\n",
    "\n",
    "3. **LLM 호출**\n",
    "   - Transformer 기반 모델이 입력 토큰을 바탕으로 응답 생성\n",
    "   - 다음 토큰을 확률적으로 예측하여 문장 완성\n",
    "\n",
    "4. **출력 생성**\n",
    "   - 토큰을 다시 텍스트로 변환\n",
    "   - 사용자가 이해 가능한 형태로 출력\n",
    "\n",
    "5. **후처리(Optional)**\n",
    "   - 응답 정제, 하이라이트, 요약, 하위 태스크 연계 등\n",
    "\n",
    "# RAG 아키텍처란?\n",
    "\n",
    "**RAG (Retrieval-Augmented Generation)**는 검색 기반 문서 생성 구조로,\n",
    "LLM의 지식 한계를 극복하기 위해 외부 문서를 참조하는 방식.\n",
    "\n",
    "## 구성 요소\n",
    "\n",
    "1. **Retriever (검색기)**:\n",
    "   - 사용자 질문을 벡터화하여, 벡터 DB에서 관련 문서를 검색\n",
    "   - ex: FAISS, Weaviate, Pinecone 등 사용\n",
    "\n",
    "2. **Generator (생성기)**:\n",
    "   - 검색된 문서를 기반으로 LLM이 정답 생성\n",
    "   - 검색 결과를 프롬프트에 포함시켜 응답 품질 향상\n",
    "\n",
    "## 장점\n",
    "- 최신 정보 반영 가능\n",
    "- 파라미터 업데이트 없이 성능 향상 가능\n",
    "- 설명 가능성(Explainability) 증가\n",
    "\n",
    "# AWS 배포 과정 정리\n",
    "\n",
    "## 1. 프로젝트 준비\n",
    "- 모델 코드, 프롬프트 파일, requirements.txt 등 정리\n",
    "- `.env` 파일은 비공개 처리\n",
    "\n",
    "## 2. 도커라이징\n",
    "- `Dockerfile` 생성\n",
    "- 모델 및 서버 실행 환경 컨테이너화\n",
    "\n",
    "## 3. 클라우드 환경 준비\n",
    "- AWS EC2 인스턴스 생성 (Ubuntu 권장)\n",
    "- 혹은 AWS SageMaker / ECS 활용 가능\n",
    "\n",
    "## 4. 배포 작업\n",
    "- EC2 접속 후 Docker 이미지 실행\n",
    "- nginx, gunicorn 등을 통한 백엔드 서비스 구동\n",
    "- 로드 밸런서나 도메인 연결(Optional)\n",
    "\n",
    "## 5. 보안 및 인증 설정\n",
    "- 보안 그룹 설정 (포트 80/443 허용)\n",
    "- SSL 인증서 적용 (Let's Encrypt 등)\n",
    "- IAM 역할 관리 및 키 보호\n",
    "\n",
    "## 6. 유지 관리\n",
    "- CloudWatch, 로그 모니터링\n",
    "- 주기적 백업 및 업데이트 자동화 스크립트 구성\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
