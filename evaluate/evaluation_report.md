# RAG 시스템 성능 평가 결과 보고서

## 1. 개요

본 보고서는 한국외국어대학교 컴퓨터공학과 RAG(Retrieval-Augmented Generation) 시스템의 성능 평가 과정과 결과를 정리한 문서입니다. 주요 평가 항목은 검색 정확도, 재정렬기 성능, 응답 품질 등이며, 테스트 데이터셋을 자동으로 매핑하는 방법을 통해 평가를 진행했습니다.

## 2. 평가 방법

### 2.1 테스트 데이터셋 ID 매핑 자동화

기존 평가 시스템의 가장 큰 문제점은 테스트 데이터셋의 `ground_truth_doc_ids`가 실제 ChromaDB에 저장된 문서 ID와 매핑되지 않아 낮은 정확도를 보인다는 점이었습니다. 이 문제를 해결하기 위해 자동 ID 매핑 기능을 개발하여 적용했습니다.

**매핑 과정:**

1. 각 질의와 정답을 ChromaDB에 검색하여 관련성 높은 문서 ID 찾기
2. 질의 기반 검색과 정답 기반 검색 결과를 중복 제거 후 병합
3. 가장 관련성 높은 문서 ID를 정답으로 설정
4. 상세 매칭 정보를 별도 파일에 저장하여 추적 가능성 확보

**관련 코드:**

```python
def auto_update_ground_truth_doc_ids(test_dataset_path, top_k=5):
    # ChromaDB에서 정답 문서 ID를 자동으로 검색하여 업데이트

    for i, (query, answer) in enumerate(zip(test_data["queries"], test_data["ground_truth_answers"])):
        # 1. 먼저 질의로 검색
        query_ids, query_chunks = find_best_match_for_answer(query, top_k)

        # 2. 정답으로 검색
        answer_ids, answer_chunks = find_best_match_for_answer(answer, top_k)

        # 3. 중복 제거 및 병합 (질의 기반 결과 우선)
        all_ids = []
        seen_ids = set()

        # 최종 정답 문서 ID 설정
        final_ids = all_ids[:3]
        updated_doc_ids.append(final_ids)
```

### 2.2 평가 영역 및 지표

평가는 다음 영역에서 진행되었습니다:

1. **Top-k 정확도 평가**: 검색된 상위 k개 문서 중 정답 문서가 포함되었는지 평가

   - 지표: Top-1, Top-3, Top-5, Top-10 정확도
   - 명령어: `python -m evaluate.evaluate --selective --top-k`

2. **Reranker 성능 평가**: 재정렬기 도입으로 인한 문서 검색 정확도 개선을 평가

   - 지표: 기존 정확도, 재정렬 후 정확도, 개선 비율
   - 명령어: `python -m evaluate.evaluate --selective --reranker`

3. **RAG 시스템 비교**: 재정렬기 적용 전/후 응답 품질 비교
   - 지표: 응답 내용, 포괄성, 정확성
   - 명령어: `python -m evaluate.evaluate --selective --comparison --query "컴퓨터공학과 졸업요건은 어떻게 되나요?"`

## 3. 평가 결과

### 3.1 Top-k 정확도 평가

자동 ID 매핑 후 Top-k 정확도 평가 결과는 다음과 같습니다:

| 지표   | 정확도       | 비고                                                |
| ------ | ------------ | --------------------------------------------------- |
| Top-1  | 100% (10/10) | 모든 쿼리에서 첫 번째 검색 결과가 정답 문서에 포함  |
| Top-3  | 100% (10/10) | 모든 쿼리에서 상위 3개 검색 결과 중 정답 문서 포함  |
| Top-5  | 100% (10/10) | 모든 쿼리에서 상위 5개 검색 결과 중 정답 문서 포함  |
| Top-10 | 100% (10/10) | 모든 쿼리에서 상위 10개 검색 결과 중 정답 문서 포함 |

> **참고**: 기존에는 정답 문서 ID와 실제 ChromaDB 문서 ID가 매핑되지 않아 Top-1(0%), Top-3(10%), Top-5(10%), Top-10(20%)의 낮은 정확도를 보였습니다.

### 3.2 Reranker 성능 평가

재정렬기(Reranker) 성능 평가 결과는 다음과 같습니다:

| 지표             | 결과         | 비고                                |
| ---------------- | ------------ | ----------------------------------- |
| 기존 정확도      | 100% (10/10) | 재정렬 전 ChromaDB 검색 정확도      |
| 재정렬 후 정확도 | 100% (10/10) | 재정렬 후 검색 정확도               |
| 개선 비율        | 0% (0/10)    | 이미 완벽한 성능이라 추가 개선 없음 |

```json
{
  "standard_hits": 10,
  "reranked_hits": 10,
  "improvements": 0,
  "total_queries": 10,
  "standard_accuracy": 1.0,
  "reranked_accuracy": 1.0,
  "improvement_rate": 0.0
}
```

### 3.3 RAG 시스템 비교

"컴퓨터공학과 졸업요건은 어떻게 되나요?"라는 샘플 질의에 대한 기존 RAG와 개선된 RAG(재정렬기 적용) 시스템의 응답 비교:

**기존 RAG 응답:**

```
컴퓨터공학과 졸업요건은 다음과 같습니다:

1. **졸업 이수학점**:
   - 전공심화: 75학점
   - 이중전공: 54학점
   - 부전공: 21학점
   - 교양: 32학점
   - 졸업학점 합계: 134학점

2. **졸업논문**:
   - 1전공 및 이중전공 학생은 졸업논문을 통과해야 졸업이 가능합니다.
   - 졸업논문 작성 및 심사는 매 학기 중반에 진행되며, 학과별로 조건 및 일정이 다를 수 있으므로 학과사무실에 확인이 필요합니다.

3. **기타 요건**:
   - 전체 성적 평점평균이 2.00 이상이어야 합니다.
   - 외국어인증 및 졸업시험 FLEX는 별도로 요구되며, 각각의 요건을 충족해야 합니다.

이 외에도 각 영역별 필수학점을 충족해야 하며, 전공으로 인정되지 않은 과목이나 잘못 분류된 교과목이 없는지 확인해야 합니다.
```

**개선된 RAG 응답:**

```
컴퓨터공학과 졸업요건은 다음과 같습니다:

1. **졸업 이수학점**: 총 134학점을 이수해야 합니다. 이는 전공, 이중전공, 부전공, 교양 학점으로 구성됩니다.
   - 전공심화 과정의 경우 75학점을 전공에서 이수해야 하며, 부전공은 선택사항입니다.
   - 이중전공의 경우 전공 54학점, 이중전공 42학점, 교양 32학점을 이수해야 합니다.

2. **졸업논문**: 1전공 및 이중전공 학생은 졸업논문을 통과해야 졸업이 가능합니다. 졸업논문은 매 학기 중반에 작성 및 심사를 진행하며, 각 학과사무실에서 조건과 일정을 확인해야 합니다.

3. **성적 요건**: 전체 성적 평점평균이 2.00 이상이어야 합니다.

4. **외국어 인증**: 외국어인증은 별도로 요구되며, 각 요건을 확인하고 처리해야 합니다.

이 외에도, 전공으로 인정되지 않는 과목은 '자유선택(자선)' 학점으로 표시되며, 이는 졸업 필수요건이 아닙니다. 그러나 영역별 최소 이수학점과 졸업학점 합계를 충족해야 졸업이 가능합니다.
```

**비교 분석:**

- 개선된 RAG 시스템의 응답이 이중전공의 상세 학점 요구사항을 더 구체적으로 설명하며, 자유선택 과목에 대한 추가 정보를 제공합니다.
- 두 시스템 모두 정확한 정보를 제공하지만, 정보의 구조화와 구체성 측면에서 개선된 시스템이 약간 더 우수합니다.

## 4. 개선점 및 한계

### 4.1 개선점

1. **테스트 데이터셋 ID 매핑 자동화**:

   - 기존 평가 시스템의 가장 큰 문제점인 정답 문서 ID 매핑 문제 해결
   - 질의와 정답 기반 검색을 결합하여 매핑 정확도 향상
   - 자동/수동 매핑 옵션으로 유연한 사용성 제공

2. **세부 매칭 정보 저장**:

   - `doc_id_matching_details.json` 파일을 통해 매핑 과정의 투명성 확보
   - 매핑 결과 디버깅 및 분석 용이

3. **성능 평가 간소화**:
   - 코드 수정 없이 명령줄 옵션으로 다양한 평가 수행 가능
   - 선택적 평가 기능으로 필요한 평가만 빠르게 수행

### 4.2 한계점

1. **순환적 평가 문제**:

   - 현재 검색 시스템의 결과를 그대로 정답으로 설정하여 시스템이 얼마나 일관되게 같은 결과를 반환하는지만 측정
   - 실제 성능을 객관적으로 평가하기 어려움

2. **과적합(Overfitting) 위험**:

   - 현재 시스템 출력을 기준으로 정답을 정의하기 때문에 평가가 시스템의 현재 상태에 과적합됨
   - 100% 정확도가 실제 성능을 의미하지 않을 수 있음

3. **신뢰성 검증 부족**:

   - 자동으로 선택된 문서가 실제로 질문에 대한 최적의 답변을 포함하는지 인간 검증 필요

4. **ROUGE/BLEU 평가 미실시**:
   - 응답 품질 평가를 위한 ROUGE/BLEU 등의 메트릭 평가 추가 필요

## 5. 향후 개선 방향

1. **외부 전문가 검증**:

   - 컴퓨터공학과 전문가가 직접 ground truth 문서와 응답을 검토 및 확인

2. **인간 평가 추가**:

   - 응답 품질에 대한 인간 평가자의 정성적 평가 도입
   - 정확성, 관련성, 완전성, 유용성 등 다차원 평가

3. **평가 데이터셋 확장**:

   - 더 다양하고 복잡한 질의를 포함한 테스트 데이터셋 구축
   - 엣지 케이스 및 어려운 질의 추가

4. **통합 자동화 테스트**:
   - 평가를 CI/CD 파이프라인에 통합하여 지속적인 성능 모니터링
   - 성능 저하 발생 시 자동 알림 시스템 구축

## 6. 결론

테스트 데이터셋 ID 매핑 자동화를 통해 RAG 시스템 평가의 정확도와 효율성을 크게 향상시켰습니다. 기존에 낮은 정확도로 평가되었던 문제가 해결되어 정확한 성능 측정이 가능해졌으며, 이를 통해 시스템 개선 방향을 더 명확하게 설정할 수 있게 되었습니다.

그러나 현재의 평가 방법은 여전히 한계가 있으며, 특히 순환적 평가 문제와 과적합 위험을 해결하기 위해 외부 전문가 검증 및 인간 평가 추가 등의 개선이 필요합니다.

이러한 개선을 통해 한국외국어대학교 컴퓨터공학과 RAG 시스템의 성능을 더욱 정확하게 평가하고, 학생들에게 더 유용한 정보를 제공할 수 있는 시스템으로 발전시켜 나갈 수 있을 것입니다.
